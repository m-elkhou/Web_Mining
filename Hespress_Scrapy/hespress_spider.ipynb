{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scrapy, csv, sys, os\n",
    "from datetime import datetime, timedelta\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HespressSpider(scrapy.Spider):\n",
    "    name = 'hespress'\n",
    "    allowed_domains = ['hespress.com']\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36'\n",
    "    # user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'\n",
    "    # user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36'\n",
    "    def clear(self):\n",
    "        # sys.stderr.flush()\n",
    "        sys.stdout.flush()\n",
    "        os.system('cls' if os.name=='nt' else 'clear')\n",
    "        f = open('log', \"w\")\n",
    "        f.close()\n",
    "\n",
    "    def start_requests(self):\n",
    "        with open(\"HespressComments.csv\",\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f,fieldnames = ['title', 'type', 'link', 'date', 'comments'])\n",
    "            writer.writeheader()\n",
    "        \n",
    "        self.bar = tqdm(1e+15, desc= \"nb comment\")#, file = sys.stdout)\n",
    "        self.comment_date = ''\n",
    "        # articles du mois septembre et octobre\n",
    "        url = \"https://www.hespress.com/archive/\"\n",
    "        date = datetime.today()\n",
    "        # date_time_str = 'Jun 31 2019  12:00PM'\n",
    "        # date = datetime.strptime(date_time_str, '%b %d %Y %I:%M%p')\n",
    "        date_time_str = '2019-12-30 08:15:27.243860'\n",
    "        date = datetime.strptime(date_time_str, '%Y-%m-%d %H:%M:%S.%f')\n",
    "         \n",
    "        for _ in range(10):\n",
    "            date -= timedelta(days=1)\n",
    "            for i in range(1,11):\n",
    "                try:\n",
    "                    url = url +str(date.year)+\"/\"+str(date.month)+\"/\"+str(date.day)+ \"/index.\"+ str(i) +\".html\"\n",
    "                    self.comment_date = str(date.year)+\"/\"+str(date.month)+\"/\"+str(date.day)\n",
    "                    self.clear()\n",
    "                    yield scrapy.Request(url = url, callback = self.getUrls)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "    def getUrls(self, response):\n",
    "        urls = response.xpath('//div[@id=\"box_center_holder\"]//div[@class=\"short\"]//h2[@class=\"section_title\"]//a/@href').extract()\n",
    "        # titres = response.xpath('//div[@id=\"box_center_holder\"]//div[@class=\"short\"]//h2[@class=\"section_title\"]//a/text()').extract()\n",
    "        # for url,titre in zip(urls,titres):\n",
    "        #     yield scrapy.Request(response.urljoin(url), self.parse)\n",
    "        # with open(r'D:/WISD/S3/Web_Mining/Scrapy_Spider/URLs.txt', 'a', encoding=\"utf-8\") as f:\n",
    "        #     f.write(\"\\n\".join(urls))\n",
    "        for url in zip(urls):\n",
    "            yield scrapy.Request(response.urljoin(str(url)), self.parse)\n",
    "          \n",
    "    def parse(self, response):\n",
    "        items = {}\n",
    "        items[\"title\"] = response.xpath('//div[@id=\"article_holder\"]//h1[@class=\"page_title\"]/text()').extract()[0]\n",
    "        # items[\"date\"]  = response.xpath('//div[@id=\"article_body\"]//div[@class=\"story_stamp\"]//spam/text()').extract()\n",
    "        # items[\"date\"]  = response.xpath('//div[@id=\"comment_list\"]//div[@class=\"comment_holder\"]//div[@class=\"irow_1\"]//div[@class=\"comment_body_in\"]//div[@class=\"comment_body\"]//div[@class=\"comment_header\"]//spam/text()').extract()\n",
    "        items[\"date\"]  = self.comment_date\n",
    "        items[\"link\"]  = response.xpath('//head//link/@href').extract()[0]\n",
    "        items[\"type\"]  = items[\"link\"].split(\"/\")[-2]\n",
    "        comments = response.xpath('//div[@id=\"comment_list\"]//div[@class=\"comment_text\"]/text()').extract()\n",
    "                \n",
    "        with open(\"HespressComments.csv\", \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames = ['title', 'type', 'link', 'date', 'comments'])\n",
    "            for c in comments:\n",
    "\n",
    "                items[\"comments\"] = str(c)\n",
    "                # if c:\n",
    "                #     items[\"comments_2\"] = str(tp().text_pros(c))\n",
    "                # else:\n",
    "                #     items[\"comments_2\"] = \"\"\n",
    "                writer.writerow(items)\n",
    "                # self.itemlist.append(items)\n",
    "\n",
    "                self.bar.update(1)\n",
    "                # yield items\n",
    "            \n",
    "        # with open(r'D:/WISD/S3/Web_Mining/Scrapy_Spider/comments.txt', 'a', encoding=\"utf-8\") as f:\n",
    "        #     f.write(\"\\n\".join(comments))        \n",
    "\n",
    "# process = CrawlerProcess({\n",
    "#     'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "# })\n",
    "\n",
    "# process.crawl(HespressSpider)\n",
    "# process.start() # the script will block here until the crawling is finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}