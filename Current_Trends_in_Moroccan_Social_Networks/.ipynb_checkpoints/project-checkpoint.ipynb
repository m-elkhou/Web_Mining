{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Web Mining - Project : #\n",
    "\n",
    "## Objectif : \n",
    "\n",
    "<p>\n",
    "Mining social media network (FaceBook, Twitter, etc.) and news websites </br>\n",
    "for discovering what topics Moroccan people are discussing during the two last years.  \n",
    "\n",
    "- Use LDA (Latent Dirichlet Allocation) or other topic identification techniques.\n",
    "\n",
    "- Provide a deep analysis.\n",
    "</p>\n",
    "\n",
    "***\n",
    "</br>\n",
    "\n",
    "### Importation des libraires :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from googletrans import Translator\n",
    "from langdetect import detect\n",
    "from textblob import TextBlob\n",
    "\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "home = 'D:/WISD/S3/Web_Mining/Current_Trends_in_Moroccan_Social_Networks/'\n",
    "\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-e01d9716e16f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'python -m scrapy runspider \"HespressSpider3.py\" --nolog'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.system('python -m scrapy runspider \"HespressSpider3.py\" --nolog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing tweets with NLP #\n",
    "\n",
    "The steps we followed to process the stored Tweets are:\n",
    "\n",
    "a) Delete unnecessary data: usernames,emails,hyperlinks, retweets, punctuation, \n",
    "possessives from a noun,duplicate characters, and special characters like smileys.\n",
    "\n",
    "b) Normalize whitespace (convert multiple sequential whitespace chars into one whitespace character).\n",
    "\n",
    "c) Convert hashtags into separate words, for example, thehashtag #MoroccanUsers \n",
    "is converted into two wordsMoroccan and Users.\n",
    "\n",
    "d) Transform words writtenin Moroccan dialect, or in a dialect of Berber Tamazight into Standard Arabic. \n",
    "These words couldbe written using the Arabic or French alphabet. \n",
    "To performthistask, we create a python file that contains a dictionary of words that we gathered, \n",
    "then we store it in each slave node of our clusterand imported inside the NLP script\n",
    "\n",
    "e) Create a function to detect the language used to write\n",
    "the text of tweet (Standard Arab, French or English).\n",
    "\n",
    "f) Create a function for automatic correction of spelling mistakes.\n",
    "\n",
    "g) Create a list of contractions to normalize and\n",
    "expandwordslike What's=>What is\n",
    "\n",
    "h) Delete the suffix of a word until we find the root. For\n",
    "example; Stemming => stem\n",
    "\n",
    "i) Remove stopwords for standard Arabic ( ,(...,Ø£Ù†, Ø¥Ù†, Ø¨Ø¹Ø¯\n",
    "French (alors, Ã , ainsi, ...), and English (about, above, almost,...).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Delete unnecessary data: usernames,emails,hyperlinks, punctuation, possessives from a noun,duplicate characters, and special characters like smileys. ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "demoji_df = pd.read_json(home + 'demoji.json', encoding='utf-8') # from https://pypi.org/project/demoji/\n",
    "demoji_df.reset_index(inplace=True)\n",
    "demoji_df.astype({\"index\": str})\n",
    "\n",
    "def delete_unnecessary_data(tweet):\n",
    "    #delete www.* or https?://* \n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet)\n",
    "    #delete @username\n",
    "    tweet = re.sub(' @[^\\s]+',' ',tweet)\n",
    "    #delete username*\n",
    "    tweet = re.sub('username[^\\s]+','',tweet)\n",
    "    #delete emails\n",
    "    tweet = re.sub('[^\\s@]+@[^\\s@]+',' ',tweet)\n",
    "\n",
    "    #delete javascript tags\n",
    "    tweet =re.sub('< *script*>.*?< *script*>',' ',tweet)\n",
    "    #delete all html tags\n",
    "    tweet = re.sub('<.*?>',' ',tweet)\n",
    "    #delete numbers \n",
    "    tweet = re.sub(\"[0-9><,]+\",\" \",tweet)\n",
    "    #delete reteur a la ligne\n",
    "    tweet = re.sub(r\"\\n+|â”Š\",\" \",tweet)\n",
    "\n",
    "    # delete duplicate characters\n",
    "    tweet = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", tweet)\n",
    "\n",
    "    # Remove special characters like smileys (imojie)\n",
    "    # ğŸ˜‚ğŸ¤·â€â™€ï¸â¤ï¸ğŸ”´ğŸ“¢âœ…âğŸ¥˜â†˜ï¸ğŸŒ»â™¥ï¸â™¥ï¸â™¥ï¸ğŸ¥µğŸ†šğŸ“…ğŸ•—ğŸ“ğŸ‘‹ğŸ˜©ğŸ˜¢ğŸ™ŒğŸ¾ğŸ”¥ğŸ˜®ğŸ’–ğŸ˜­ğŸ‘„â¤ğŸ¤¢ğŸ’¥ğŸ’£\n",
    "    # ğŸ„â¤â¤ğŸªğŸ±ğŸ’°ğŸ·â­ğŸ™„ğŸ˜ğŸ™ŒğŸ‘‡ğŸ’šğŸ˜­ğŸ˜¹ğŸŒ¸ğŸ’›ğŸ™ğŸ‘ğŸ˜”ğŸğŸ¥°â„ğŸ„ğŸ’¤\n",
    "     \n",
    "    # tweet = re.sub(r'ğŸ˜‚|ğŸ¤·â€â™€ï¸|â¤ï¸|ğŸ”´|ğŸ“¢|âœ…|â|ğŸ¥˜|â†˜ï¸|ğŸŒ»|â™¥ï¸ï¸|ï¸ğŸ¥µ|ğŸ†š|ğŸ“…|ğŸ•—|ğŸ“|ğŸ‘‹|ğŸ˜©|ğŸ˜¢|ğŸ™ŒğŸ¾|ğŸ”¥|ğŸ˜®|ğŸ’–|ğŸ˜­|ğŸ‘„|â¤|ğŸ¤¢|ğŸ’¥|ğŸ’£','',tweet)\n",
    "    # Stage el wa7che\n",
    "    # tweet = tweet.encode('ascii', 'ignore').decode('ascii')\n",
    "   \n",
    "    for code in demoji_df[\"index\"]:\n",
    "        try:\n",
    "            tweet = re.sub(code, '', tweet)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDownloading emoji data ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m (Got response in 3.40 seconds)\n",
      "\u001b[33mWriting emoji data to C:\\Users\\mhmh2\\.demoji/codes.json ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import demoji\n",
    "demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#âƒ£</td>\n",
       "      <td>2020-01-05 20:19:37.340117693</td>\n",
       "      <td>keycap: #</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#ï¸âƒ£</td>\n",
       "      <td>2020-01-05 20:19:37.340117693</td>\n",
       "      <td>keycap: #</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>*âƒ£</td>\n",
       "      <td>2020-01-05 20:19:37.340117693</td>\n",
       "      <td>keycap: *</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>*ï¸âƒ£</td>\n",
       "      <td>2020-01-05 20:19:37.340117693</td>\n",
       "      <td>keycap: *</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0âƒ£</td>\n",
       "      <td>2020-01-05 20:19:37.340117693</td>\n",
       "      <td>keycap: 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3831</th>\n",
       "      <td>ğŸª‘</td>\n",
       "      <td>2020-01-05 20:19:37.340117693</td>\n",
       "      <td>chair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3832</th>\n",
       "      <td>ğŸª’</td>\n",
       "      <td>2020-01-05 20:19:37.340117693</td>\n",
       "      <td>razor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3833</th>\n",
       "      <td>ğŸª“</td>\n",
       "      <td>2020-01-05 20:19:37.340117693</td>\n",
       "      <td>axe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3834</th>\n",
       "      <td>ğŸª”</td>\n",
       "      <td>2020-01-05 20:19:37.340117693</td>\n",
       "      <td>diya lamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3835</th>\n",
       "      <td>ğŸª•</td>\n",
       "      <td>2020-01-05 20:19:37.340117693</td>\n",
       "      <td>banjo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3836 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                     timestamp      codes\n",
       "0       #âƒ£ 2020-01-05 20:19:37.340117693  keycap: #\n",
       "1      #ï¸âƒ£ 2020-01-05 20:19:37.340117693  keycap: #\n",
       "2       *âƒ£ 2020-01-05 20:19:37.340117693  keycap: *\n",
       "3      *ï¸âƒ£ 2020-01-05 20:19:37.340117693  keycap: *\n",
       "4       0âƒ£ 2020-01-05 20:19:37.340117693  keycap: 0\n",
       "...    ...                           ...        ...\n",
       "3831     ğŸª‘ 2020-01-05 20:19:37.340117693      chair\n",
       "3832     ğŸª’ 2020-01-05 20:19:37.340117693      razor\n",
       "3833     ğŸª“ 2020-01-05 20:19:37.340117693        axe\n",
       "3834     ğŸª” 2020-01-05 20:19:37.340117693  diya lamp\n",
       "3835     ğŸª• 2020-01-05 20:19:37.340117693      banjo\n",
       "\n",
       "[3836 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('C:/Users/mhmh2/.demoji/codes.json')\n",
    "df.reset_index(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   Ø±Ù‚ÙŠ  Ø§Ø¨Ø¯Ø§Ø§Ø¹    Ø°ÙˆÙˆÙ‚   ÙØ®Ø§Ù…Ø©  Ø§Ù†Ø§Ù‚Ù‡.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"â”Šâœ¨ â”Š ğŸ’§Ø±Ù‚ÙŠ â”ŠğŸƒğŸŒŸğŸ’§Ø§Ø¨Ø¯Ø§Ø§Ø§Ø¹â”ŠğŸŒŸâ”Šâœ¨â”ŠğŸŒŸâ”ŠğŸ’§Ø°ÙˆÙˆÙˆÙ‚â”ŠğŸƒâ”ŠğŸŒŸâ”ŠğŸ’§ÙØ®Ø§Ù…Ø©â”Šâœ¨ğŸŒŸâ”ŠğŸ’§Ø§Ù†Ø§Ù‚Ù‡.\"\n",
    "# text =' â”Š test'\n",
    "# text = ' tdg@bkja \"Ù‚ÙˆÙ„ Ø¨Ø°Ù„Øª Ø¬Ù‡Ø¯ÙŠ ÙˆØ§Ù„Ø¨Ø§Ù‚ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù„Ù‡ØŸ ğŸ˜¹ğŸ˜¹ğŸ˜¹ğŸ˜¹ https://t.co/YalhezFnsl \"#acsjjfac !\\n @ # % ^ & * ( ğŸ“¢âœ…âğŸ¥˜â†˜ï¸ğŸŒ»â™¥ï¸â™¥ï¸â™¥ï¸ğŸ¥µğŸ†šğŸ“…ğŸ•—ğŸ“ğŸ‘‹ğŸ˜©ğŸ˜¢ğŸ™ŒğŸ¾ğŸ”¥ğŸ˜®ğŸ’–ğŸ˜­ğŸ‘„â¤ğŸ¤¢ğŸ’¥ğŸ’£'\n",
    "text = delete_unnecessary_data(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Normalize whitespace (convert multiple sequential whitespace chars into one whitespace character). ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_whitespace(tweet):\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Ø±Ù‚ÙŠ Ø§Ø¨Ø¯Ø§Ø§Ø¹ Ø°ÙˆÙˆÙ‚ ÙØ®Ø§Ù…Ø© Ø§Ù†Ø§Ù‚Ù‡.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = normalize_whitespace(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Convert hashtags into separate words, for example, thehashtag #MoroccanUsers is converted into two words Moroccan and Users. ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_h(hashtagestring):\n",
    "    fo = re.compile(r'#[A-Z]{2,}(?![a-z])|[A-Z][a-z]+')\n",
    "    fi = fo.findall(hashtagestring)\n",
    "    return ' '.join(fi)\n",
    "\n",
    "def split_hashtage(tweet):\n",
    "    tweet = re.sub(r'#[^\\s]+', lambda m: sp_h(m.group()), tweet) # #WakeUpMorocco => Wake Up Morocco\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata as ud\n",
    "import string\n",
    "\n",
    "def reamove_punctuation(tweet):\n",
    "#     regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "#     tweet = regex.sub('', tweet)\n",
    "#     tweet = re.sub(ud.category(c).startswith('P'),'',tweet)\n",
    "\n",
    "    tweet = re.sub('[^\\w\\s\\']',' ',tweet)\n",
    "#     tweet = ''.join(c for c in tweet if not ud.category(c).startswith('P'))\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) Create a function to detect the language used to write the text of tweet (Standard Arab, French or English). ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "from langdetect import detect\n",
    "from textblob import TextBlob\n",
    "\n",
    "def language_detction(tweet):\n",
    "    try:\n",
    "        lang = translator.detect(tweet).lang\n",
    "    except:\n",
    "        try:\n",
    "            lang = detect(tweet)\n",
    "        except:\n",
    "            try:\n",
    "                lang = str(TextBlob(tweet).detect_language())\n",
    "            except:\n",
    "                lang = 'unknown'\n",
    "    \n",
    "    if lang in ['ar', 'arfa', 'fa', 'faar'] :\n",
    "        tweet = correct_ar(tweet)\n",
    "    elif lang == 'fr':\n",
    "        tweet = correct_fr(tweet)\n",
    "    elif lang == 'en':\n",
    "        tweet = correct_en(tweet)\n",
    "    else:\n",
    "        tweet = translator.translate(tweet,src='auto',dest='en' ).text\n",
    "        tweet = correct_en(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhh je suis encore vivant \t\t hhh I'm still alive \t\t Detected(lang=fr, confidence=1.0)\n"
     ]
    }
   ],
   "source": [
    "# text = 'chkon nta'\n",
    "# text = \"Ø§ØªØ¨Ø§Ø±Ùƒ Ø§Ù„Ù„Ù‡ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø§Ø¬ Ø±Ø§Ùƒ ØªÙ…Ø§.Ù‡Ù‡\"\n",
    "text = 'hhh je suis encore vivant'\n",
    "# text ='ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† ØªØ¹Ø²ÙŠØ² ÙƒÙØ§Ø¡Ø§Øª Ù‡Ø°Ø§ Ø§Ù„Ø¬Ù‡Ø§Ø² Ø¨ØªØ·ÙˆÙŠØ±Ù‡ Ø¨Ø¢Ø³ØªØ®Ø°Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø¥ØµØ·Ù†Ø§Ø¹ÙŠ ØŸ Ù…Ø«Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø¥Ø³ØªØ¹Ù…Ø§Ù„ Ù‚Ù„Ù… ÙŠØºÙŠØ± Ù‚Ø·Ø± Ø§Ù„ÙƒØªØ§Ø¨Ø© Ø£Ùˆ ØªØºÙŠÙŠØ± Ù„ÙˆÙ† Ø§Ù„Ø­Ø¨Ø± Ø¨Ø¯ÙˆÙ† ØªØºÙŠÙŠØ± Ø§Ù„Ù‚Ù„Ù… ÙÙ‚Ø· Ø¨Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ø£ÙˆØ§Ù…Ø±ØŒ Ù…Ø§Ø°Ø§ Ø¹Ù†ÙƒÙ… ØŸ'\n",
    "dt = translator.detect(text)\n",
    "a = translator.translate(text,src='auto',dest='en' ).text\n",
    "print(text , \"\\t\\t\", a , \"\\t\\t\", dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f) Create a function for automatic correction of spelling mistakes. ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller\n",
    "spell = Speller(lang='en')\n",
    "\n",
    "def correct_en(text):\n",
    "    #convert to lower case\n",
    "    text = text.lower()\n",
    "    # Clean the text\n",
    "    text = re.sub(\"\\'s \", \" is\", text) # we have cases like \"Sam is\" or \"Sam's\" (i.e. his) these two cases aren't separable, I choose to compromise are kill \"'s\" directly\n",
    "    text = re.sub(r\" whats \", \" what is \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(\"n\\'t\", \" not \", text)\n",
    "    text = re.sub(\"i'm\", \"i am\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'re\", \" are \", text)\n",
    "    text = re.sub(\"\\'d\", \" would \", text)\n",
    "    text = re.sub(\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(\"e-mail\", \" email \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\(s\\)\", \" \", text, flags=re.IGNORECASE) #mester(s)\n",
    "    text = re.sub(r\" (the[\\s]+|The[\\s]+)?(us(a)?|u\\.s\\.(a\\.)?|united state(s)?) \", \" america \", text)\n",
    "    text = re.sub(r\" uk \", \" england \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" imrovement \", \" improvement \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" intially \", \" initially \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" dms \", \" direct messages \", text, flags=re.IGNORECASE)  \n",
    "    text = re.sub(r\" demonitization \", \" demonetization \", text, flags=re.IGNORECASE) \n",
    "    text = re.sub(r\" actived \", \" active \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" kms \", \" kilometers \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" cs \", \" computer science \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" calender \", \" calendar \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" ios \", \" operating system \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" programing \", \" programming \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" bestfriend \", \" best friend \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"bn8|god8\" ,'good night', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" 2moro | 2mrrw | 2morrow | 2mrw | tomrw \", \" tomorrow \", text)\n",
    "    text = re.sub(r\" b4 \", \" before \", text)\n",
    "    text = re.sub(r\" otw \", \" on the way \", text)\n",
    "\n",
    "    text = spell(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def correct_fr(text):\n",
    "    text = TextBlob(text).correct()\n",
    "    return str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je manger le banana et la pot\n"
     ]
    }
   ],
   "source": [
    "print(correct_fr('je mangee le banane et la pom'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Transforming words written in Moroccan dialect ##\n",
    "\n",
    "(by the French alphabet or Arabic) and also written in a dialect of Berber Tamazight (by the French alphabet into the standard Arabic. \n",
    "\n",
    "For this reason, we create a dictionary of words that we gathered in a python file to perform this task, This file will be stored in each slave node of our cluster, and it will be imported in the NLP script executed in these nodes, the file looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Arabic</th>\n",
       "      <th>Transcribed Moroccan Arabic</th>\n",
       "      <th>Moroccan Darija in the Arabic Alphabet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Ù†Ø¹Ù…</td>\n",
       "      <td>Iyyeh</td>\n",
       "      <td>Ø¥ÙŠÙŠÙ‡</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Ù†Ø¹Ù…</td>\n",
       "      <td>ah</td>\n",
       "      <td>Ø¢Ù‡</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Ù†Ø¹Ù…</td>\n",
       "      <td>wah</td>\n",
       "      <td>ÙˆØ§Ù‡</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No</td>\n",
       "      <td>Ù„Ø§</td>\n",
       "      <td>Lla</td>\n",
       "      <td>Ù„Ø§</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Please</td>\n",
       "      <td>Ù…Ù† ÙØ¶Ù„Ùƒ</td>\n",
       "      <td>3afak</td>\n",
       "      <td>Ø¹Ø§ÙØ§Ùƒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Thanks</td>\n",
       "      <td>Ø´ÙƒØ±Ø§</td>\n",
       "      <td>Shokran</td>\n",
       "      <td>Ø´ÙƒØ±Ø§</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I love you</td>\n",
       "      <td>Ø§Ù†Ø§ Ø§Ø­Ø¨Ùƒ</td>\n",
       "      <td>Kanbghik</td>\n",
       "      <td>ÙƒÙ†Ø¨ØºÙŠÙƒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I miss you</td>\n",
       "      <td>Ø§Ø´ØªÙ‚Øª Ø§Ù„ÙŠÙƒ</td>\n",
       "      <td>Twe77eshtek</td>\n",
       "      <td>ØªÙˆØ­Ø´ØªÙƒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A lot</td>\n",
       "      <td>ÙƒØ«ÙŠØ±</td>\n",
       "      <td>Bezzaf</td>\n",
       "      <td>Ø¨Ø²Ø§Ù</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A little</td>\n",
       "      <td>Ù‚Ù„ÙŠÙ„</td>\n",
       "      <td>Shwiya</td>\n",
       "      <td>Ø´ÙˆÙŠØ©</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      English      Arabic Transcribed Moroccan Arabic  \\\n",
       "0         Yes         Ù†Ø¹Ù…                       Iyyeh   \n",
       "1         Yes         Ù†Ø¹Ù…                          ah   \n",
       "2         Yes         Ù†Ø¹Ù…                         wah   \n",
       "3          No          Ù„Ø§                         Lla   \n",
       "4      Please     Ù…Ù† ÙØ¶Ù„Ùƒ                       3afak   \n",
       "5      Thanks        Ø´ÙƒØ±Ø§                     Shokran   \n",
       "6  I love you    Ø§Ù†Ø§ Ø§Ø­Ø¨Ùƒ                    Kanbghik   \n",
       "7  I miss you  Ø§Ø´ØªÙ‚Øª Ø§Ù„ÙŠÙƒ                 Twe77eshtek   \n",
       "8       A lot        ÙƒØ«ÙŠØ±                      Bezzaf   \n",
       "9    A little        Ù‚Ù„ÙŠÙ„                      Shwiya   \n",
       "\n",
       "  Moroccan Darija in the Arabic Alphabet  \n",
       "0                                   Ø¥ÙŠÙŠÙ‡  \n",
       "1                                     Ø¢Ù‡  \n",
       "2                                    ÙˆØ§Ù‡  \n",
       "3                                     Ù„Ø§  \n",
       "4                                  Ø¹Ø§ÙØ§Ùƒ  \n",
       "5                                   Ø´ÙƒØ±Ø§  \n",
       "6                                 ÙƒÙ†Ø¨ØºÙŠÙƒ  \n",
       "7                                 ØªÙˆØ­Ø´ØªÙƒ  \n",
       "8                                   Ø¨Ø²Ø§Ù  \n",
       "9                                   Ø´ÙˆÙŠØ©  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ma_df = pd.read_csv( home + \"Moroccan_dialect.csv\", sep=';',encoding='utf-8')\n",
    "word_ma_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ahramon Ø§Ù„Ø·Ø¨ Ø§Ù„Ø¨ÙŠØ·Ø±ÙŠ Ù…Ù† ÙØ¶Ù„Ùƒ Ù†Ø¹Ù… Ù†Ø¹Ù… Ø£Ø¹Ø·Ù†ÙŠ '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def moroccan_dialect(tweet):\n",
    "    tweet= ' '+tweet+' '\n",
    "    for ligne in word_ma_df.values:\n",
    "        tweet = re.sub(r' '+str(ligne[2])+' | '+str(ligne[3])+' ', ' '+str(ligne[1])+' ', tweet)\n",
    "    return tweet\n",
    "\n",
    "tweet = 'ahramon Ettebb elbaytari 3afak Ø¥ÙŠÙŠÙ‡ wah 3tini'\n",
    "tweet = moroccan_dialect(tweet)\n",
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Use scrapy to scrape 'http://mylanguages.org/moroccan_vocabulary.php'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>ar</th>\n",
       "      <th>mafr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>numbers</td>\n",
       "      <td>Ø£Ø±Ù‚Ø§Ù…</td>\n",
       "      <td>nmari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>one</td>\n",
       "      <td>ÙˆØ§Ø­Ø¯</td>\n",
       "      <td>wahade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>two</td>\n",
       "      <td>Ø§Ø«Ù†Ø§Ù†</td>\n",
       "      <td>zouje</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>three</td>\n",
       "      <td>Ø«Ù„Ø§Ø«Ø©</td>\n",
       "      <td>tlata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>four</td>\n",
       "      <td>Ø£Ø±Ø¨Ø¹Ø©</td>\n",
       "      <td>rabaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>five</td>\n",
       "      <td>Ø®Ù…Ø³Ø©</td>\n",
       "      <td>khamssa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>six</td>\n",
       "      <td>Ø³ØªØ©</td>\n",
       "      <td>satta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a green tree</td>\n",
       "      <td>Ø´Ø¬Ø±Ø© Ø®Ø¶Ø±Ø§Ø¡</td>\n",
       "      <td>shajra khdra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a tall building</td>\n",
       "      <td>Ù…Ø¨Ù†Ù‰ Ø·ÙˆÙŠÙ„ Ø§Ù„Ù‚Ø§Ù…Ø©</td>\n",
       "      <td>imara aliya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a very old man</td>\n",
       "      <td>Ø±Ø¬Ù„ ÙƒØ¨ÙŠØ± Ø¬Ø¯Ø§</td>\n",
       "      <td>rajale  sharafe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                en                ar             mafr\n",
       "0          numbers             Ø£Ø±Ù‚Ø§Ù…            nmari\n",
       "1              one              ÙˆØ§Ø­Ø¯           wahade\n",
       "2              two             Ø§Ø«Ù†Ø§Ù†            zouje\n",
       "3            three             Ø«Ù„Ø§Ø«Ø©            tlata\n",
       "4             four             Ø£Ø±Ø¨Ø¹Ø©            rabaa\n",
       "5             five              Ø®Ù…Ø³Ø©          khamssa\n",
       "6              six               Ø³ØªØ©            satta\n",
       "7     a green tree        Ø´Ø¬Ø±Ø© Ø®Ø¶Ø±Ø§Ø¡     shajra khdra\n",
       "8  a tall building  Ù…Ø¨Ù†Ù‰ Ø·ÙˆÙŠÙ„ Ø§Ù„Ù‚Ø§Ù…Ø©     imara aliya \n",
       "9   a very old man      Ø±Ø¬Ù„ ÙƒØ¨ÙŠØ± Ø¬Ø¯Ø§  rajale  sharafe"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ma_df2 = pd.read_csv( home + \"mylanguages.csv\", sep=',',encoding='utf-8')\n",
    "word_ma_df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moroccan_dialect2(tweet):\n",
    "    tweet= ' '+tweet+' '\n",
    "    for ligne in word_ma_df.values:\n",
    "        tweet = re.sub(r' '+str(ligne[2])+' ', ' '+str(ligne[1])+' ', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ahramon Ø§Ù„Ø·Ø¨ Ø§Ù„Ø¨ÙŠØ·Ø±ÙŠ Ù…Ù† ÙØ¶Ù„Ùƒ Ù†Ø¹Ù… Ù†Ø¹Ù… Ø§Ø¹Ø·Ù†ÙŠ '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from https://alraqmiyyat.github.io/2013/01-02.html\n",
    "def normalizeArabic(text):\n",
    "    text = re.sub(\"[Ø¥Ø£Ù±Ø¢Ø§]\", \"Ø§\", text)\n",
    "    text = re.sub(\"Ù‰\", \"ÙŠ\", text)\n",
    "    text = re.sub(\"Ø¤\", \"Ø¡\", text)\n",
    "    text = re.sub(\"Ø¦\", \"Ø¡\", text)\n",
    "    return(text)\n",
    "\n",
    "normalizeArabic(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø¥Ù† Ø§Ù„Ù‚Ø±Ø§Ø¡ ÙŠÙ‚Ø±Ø¤ÙˆÙ† Ø§Ù„Ù‚Ø±Ø¢Ù† Ù‚Ø±Ø§Ø¡Ø© Ø¬Ù…ÙŠÙ„Ø©\n"
     ]
    }
   ],
   "source": [
    "# from https://alraqmiyyat.github.io/2013/01-02.html\n",
    "def deNoise(text):\n",
    "    noise = re.compile(\"\"\" Ù‘    | # Tashdid\n",
    "                             Ù    | # Fatha\n",
    "                             Ù‹    | # Tanwin Fath\n",
    "                             Ù    | # Damma\n",
    "                             ÙŒ    | # Tanwin Damm\n",
    "                             Ù    | # Kasra\n",
    "                             Ù    | # Tanwin Kasr\n",
    "                             Ù’    | # Sukun\n",
    "                             Ù€     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "    text = re.sub(noise, '', text)\n",
    "    return text\n",
    "\n",
    "testLine = \"Ø¥ÙÙ†ÙÙ‘ Ø§Ù„Ù’Ù‚ÙØ±ÙÙ‘Ø§Ù’Ø¡Ù ÙŠÙÙ‚Ù’Ø±ÙØ¤ÙÙˆÙ’Ù†Ù Ø§Ù„Ù’Ù‚ÙØ±Ù’Ø¢Ù†Ù Ù‚ÙØ±ÙØ§Ù’Ø¡ÙØ©Ù‹ Ø¬ÙÙ…ÙÙŠÙ’Ù„ÙÙ€Ù€Ù€Ù€Ù€Ù€Ø©Ù‹\"\n",
    "print(deNoise(testLine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyarabic'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-d4820944f6bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyarabic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maraby\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0maraby\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyarabic'"
     ]
    }
   ],
   "source": [
    "import pyarabic.araby as araby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_ar(text):\n",
    "    text = moroccan_dialect(text)\n",
    "    text = moroccan_dialect2(text)\n",
    "    text = normalizeArabic(text)\n",
    "    text = deNoise(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('arabic')\n",
    "with open(\"arabic_stop_words.txt\",\"r\", newline=\"\",encoding=\"utf-8\") as f:        \n",
    "    for l in f:\n",
    "        l = re.sub(r\"\\n+\",'',l)\n",
    "        stop_words.append(l)\n",
    "\n",
    "stop_words.extend(stopwords.words('french'))\n",
    "stop_words.extend(stopwords.words('english'))\n",
    "stop_words.extend(gensim.parsing.preprocessing.STOPWORDS)\n",
    "stop_words.extend(['tout','le','la'])\n",
    "stop_words = set(stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(lmtzr.lemmatize(text, pos='v'))\n",
    "\n",
    "\n",
    "def tokenize_lemmatize_stemming(text):\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    #replace multiple spaces with one space\n",
    "    text = re.sub(r'[\\s]+',' ',text)\n",
    "    #transfer text to lowercase\n",
    "    text = text.lower() \n",
    "    # tokenaze text\n",
    "    tokens = re.split(\" \", text)\n",
    "\n",
    "    # Remove stop words \n",
    "    result = []\n",
    "    for token in tokens :\n",
    "        if token not in stop_words and len(token) > 1:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweet):\n",
    "    tweet = delete_unnecessary_data(tweet)\n",
    "    tweet = normalize_whitespace(tweet)\n",
    "    tweet = split_hashtage(tweet)\n",
    "    tweet = reamove_punctuation(tweet)\n",
    "    tweet = language_detction(tweet)\n",
    "#     return tweet\n",
    "    tokens = tokenize_lemmatize_stemming(tweet)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"â”Šâœ¨ â”Š ğŸ’§Ø±Ù‚ÙŠ â”ŠğŸƒğŸŒŸğŸ’§Ø§Ø¨Ø¯Ø§Ø§Ø§Ø¹â”ŠğŸŒŸâ”Šâœ¨â”ŠğŸŒŸâ”ŠğŸ’§Ø°ÙˆÙˆÙˆÙ‚â”ŠğŸƒâ”ŠğŸŒŸâ”ŠğŸ’§ÙØ®Ø§Ù…Ø©â”Šâœ¨ğŸŒŸâ”ŠğŸ’§Ø§Ù†Ø§Ù‚Ù‡.\"\n",
    "# text = '   Ø±Ù‚ÙŠ  Ø§Ø¨Ø¯Ø§Ø§Ø¹    Ø°ÙˆÙˆÙ‚   ÙØ®Ø§Ù…Ø©  Ø§Ù†Ø§Ù‚Ù‡.'\n",
    "text =' RT @PsiliLemonia: Î ÏÏ‚ Î½Î± Î²Î¬Î»ÎµÎ¹Ï‚ Ï†Ï‰Ï„Î¹Î¬ ÏƒÏ„Î¿ ÎºÏÎµÎ²Î¬Ï„Î¹ ÏƒÎ±Ï‚ Î¼Îµ ÎµÏ†Ï„Î¬ Î±Ï€Î»Î­Ï‚ ÎºÎ¹Î½Î®'\n",
    "# text = \"Se realmente quisessem estar do meu lado iam estar, e quem tÃ¡ jÃ¡ basta viu!\"\n",
    "# text = preprocess(\"Ù‚ÙˆÙ„ Ø¨Ø°Ù„Øª Ø¬Ù‡Ø¯ÙŠ ÙˆØ§Ù„Ø¨Ø§Ù‚ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù„Ù‡ØŸ ğŸ˜¹ğŸ˜¹ğŸ˜¹ğŸ˜¹ https://t.co/YalhezFnsl \")\n",
    "text = preprocess(text)\n",
    "# print(delete_unnecessary_data(\"Ù‚ÙˆÙ„ Ø¨Ø°Ù„Øª Ø¬Ù‡Ø¯ÙŠ ÙˆØ§Ù„Ø¨Ø§Ù‚ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù„Ù‡ØŸ ğŸ˜¹ğŸ˜¹ğŸ˜¹ğŸ˜¹ https://t.co/YalhezFnsl \"))\n",
    "print(text)\n",
    "\n",
    "# dt = translator.detect(text).lang\n",
    "\n",
    "# tweet = translator.translate(text,src='auto',dest='en' ).text\n",
    "# print(text , \"\\n\",dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv(home+'tweets2.csv',encoding='utf-8')\n",
    "tweets_df.columns = ['user','date','text']\n",
    "tweets_df.astype({'text': str})\n",
    "\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = tweets_df['text'][1]\n",
    "# tweet = '@jawaddelycee Bah ouais logique bg aaaaaaa'\n",
    "preprocess(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = []\n",
    "# for tweet in tqdm(tweets_df['text']):\n",
    "#     tokens.append(preprocess(tweet))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.machinelearningplus.com/nlp/gensim-tutorial/\n",
    "\n",
    "## How to create Topic Models with LDA?\n",
    "\n",
    "<p>\n",
    "The objective of topic models is to extract the underlying topics from a given collection of text documents. Each document in the text is considered as a combination of topics and each topic is considered as a combination of related words.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Topic modeling can be done by algorithms like Latent Dirichlet Allocation (LDA) and Latent Semantic Indexing (LSI).\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "In both cases you need to provide the number of topics as input. The topic model, in turn, will provide the topic keywords for each topic and the percentage contribution of topics in each document.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "The quality of topics is highly dependent on the quality of text processing and the number of topics you provide to the algorithm. The earlier post on how to build best topic models explains the procedure in more detail. However, I recommend understanding the basic steps involved and the interpretation in the example below.\n",
    "</p>\n",
    "\n",
    "\n",
    "### Step 0: Load the necessary packages and import the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
