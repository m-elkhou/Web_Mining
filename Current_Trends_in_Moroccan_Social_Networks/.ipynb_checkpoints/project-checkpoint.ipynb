{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Web Mining - Project : #\n",
    "\n",
    "## Objectif : \n",
    "\n",
    "<p>\n",
    "Mining social media network (FaceBook, Twitter, etc.) and news websites </br>\n",
    "for discovering what topics Moroccan people are discussing during the two last years.  \n",
    "\n",
    "- Use LDA (Latent Dirichlet Allocation) or other topic identification techniques.\n",
    "\n",
    "- Provide a deep analysis.\n",
    "</p>\n",
    "\n",
    "***\n",
    "</br>\n",
    "\n",
    "### Importation des libraires :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from googletrans import Translator\n",
    "from langdetect import detect\n",
    "from textblob import TextBlob\n",
    "\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "home = 'D:/WISD/S3/Web_Mining/Current_Trends_in_Moroccan_Social_Networks/'\n",
    "\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-e01d9716e16f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'python -m scrapy runspider \"HespressSpider3.py\" --nolog'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.system('python -m scrapy runspider \"HespressSpider3.py\" --nolog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing tweets with NLP #\n",
    "\n",
    "The steps we followed to process the stored Tweets are:\n",
    "\n",
    "a) Delete unnecessary data: usernames,emails,hyperlinks, retweets, punctuation, \n",
    "possessives from a noun,duplicate characters, and special characters like smileys.\n",
    "\n",
    "b) Normalize whitespace (convert multiple sequential whitespace chars into one whitespace character).\n",
    "\n",
    "c) Convert hashtags into separate words, for example, thehashtag #MoroccanUsers \n",
    "is converted into two wordsMoroccan and Users.\n",
    "\n",
    "d) Transform words writtenin Moroccan dialect, or in a dialect of Berber Tamazight into Standard Arabic. \n",
    "These words couldbe written using the Arabic or French alphabet. \n",
    "To performthistask, we create a python file that contains a dictionary of words that we gathered, \n",
    "then we store it in each slave node of our clusterand imported inside the NLP script\n",
    "\n",
    "e) Create a function to detect the language used to write\n",
    "the text of tweet (Standard Arab, French or English).\n",
    "\n",
    "f) Create a function for automatic correction of spelling mistakes.\n",
    "\n",
    "g) Create a list of contractions to normalize and\n",
    "expandwordslike What's=>What is\n",
    "\n",
    "h) Delete the suffix of a word until we find the root. For\n",
    "example; Stemming => stem\n",
    "\n",
    "i) Remove stopwords for standard Arabic ( ,(...,ÿ£ŸÜ, ÿ•ŸÜ, ÿ®ÿπÿØ\n",
    "French (alors, √†, ainsi, ...), and English (about, above, almost,...).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Delete unnecessary data: usernames,emails,hyperlinks, punctuation, possessives from a noun,duplicate characters, and special characters like smileys. ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "demoji_df = pd.read_json(home + 'demoji.json', encoding='utf-8') # from https://pypi.org/project/demoji/\n",
    "demoji_df.reset_index(inplace=True)\n",
    "demoji_df.astype({\"index\": str})\n",
    "\n",
    "def delete_unnecessary_data(tweet):\n",
    "    #delete www.* or https?://* \n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet)\n",
    "    #delete @username\n",
    "    tweet = re.sub(' @[^\\s]+',' ',tweet)\n",
    "    #delete username*\n",
    "    tweet = re.sub('username[^\\s]+','',tweet)\n",
    "    #delete emails\n",
    "    tweet = re.sub('[^\\s@]+@[^\\s@]+',' ',tweet)\n",
    "\n",
    "    #delete javascript tags\n",
    "    tweet =re.sub('< *script*>.*?< *script*>',' ',tweet)\n",
    "    #delete all html tags\n",
    "    tweet = re.sub('<.*?>',' ',tweet)\n",
    "    #delete numbers \n",
    "    tweet = re.sub(\"[0-9><,]+\",\" \",tweet)\n",
    "    #delete reteur a la ligne\n",
    "    tweet = re.sub(r\"\\n+|‚îä\",\" \",tweet)\n",
    "\n",
    "    # delete duplicate characters\n",
    "    tweet = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", tweet)\n",
    "\n",
    "    # Remove special characters like smileys (imojie)\n",
    "    # üòÇü§∑‚Äç‚ôÄÔ∏è‚ù§Ô∏èüî¥üì¢‚úÖ‚ùéü•ò‚ÜòÔ∏èüåª‚ô•Ô∏è‚ô•Ô∏è‚ô•Ô∏èü•µüÜöüìÖüïóüìçüëãüò©üò¢üôåüèæüî•üòÆüíñüò≠üëÑ‚ù§ü§¢üí•üí£\n",
    "    # üéÑ‚ù§‚ù§üê™üê±üí∞üè∑‚≠êüôÑüòçüôåüëáüíöüò≠üòπüå∏üíõüôèüëèüòîüéÅü•∞‚ùÑüéÑüí§\n",
    "     \n",
    "    # tweet = re.sub(r'üòÇ|ü§∑‚Äç‚ôÄÔ∏è|‚ù§Ô∏è|üî¥|üì¢|‚úÖ|‚ùé|ü•ò|‚ÜòÔ∏è|üåª|‚ô•Ô∏èÔ∏è|Ô∏èü•µ|üÜö|üìÖ|üïó|üìç|üëã|üò©|üò¢|üôåüèæ|üî•|üòÆ|üíñ|üò≠|üëÑ|‚ù§|ü§¢|üí•|üí£','',tweet)\n",
    "    # Stage el wa7che\n",
    "    # tweet = tweet.encode('ascii', 'ignore').decode('ascii')\n",
    "   \n",
    "    for code in demoji_df[\"index\"]:\n",
    "        try:\n",
    "            tweet = re.sub(code, '', tweet)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDownloading emoji data ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m (Got response in 3.40 seconds)\n",
      "\u001b[33mWriting emoji data to C:\\Users\\mhmh2\\.demoji/codes.json ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import demoji\n",
    "demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#‚É£</td>\n",
       "      <td>2020-01-05 20:19:37.340117693</td>\n",
       "      <td>keycap: #</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#Ô∏è‚É£</td>\n",
       "      <td>2020-01-05 20:19:37.340117693</td>\n",
       "      <td>keycap: #</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>*‚É£</td>\n",
       "      <td>2020-01-05 20:19:37.340117693</td>\n",
       "      <td>keycap: *</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>*Ô∏è‚É£</td>\n",
       "      <td>2020-01-05 20:19:37.340117693</td>\n",
       "      <td>keycap: *</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0‚É£</td>\n",
       "      <td>2020-01-05 20:19:37.340117693</td>\n",
       "      <td>keycap: 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3831</th>\n",
       "      <td>ü™ë</td>\n",
       "      <td>2020-01-05 20:19:37.340117693</td>\n",
       "      <td>chair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3832</th>\n",
       "      <td>ü™í</td>\n",
       "      <td>2020-01-05 20:19:37.340117693</td>\n",
       "      <td>razor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3833</th>\n",
       "      <td>ü™ì</td>\n",
       "      <td>2020-01-05 20:19:37.340117693</td>\n",
       "      <td>axe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3834</th>\n",
       "      <td>ü™î</td>\n",
       "      <td>2020-01-05 20:19:37.340117693</td>\n",
       "      <td>diya lamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3835</th>\n",
       "      <td>ü™ï</td>\n",
       "      <td>2020-01-05 20:19:37.340117693</td>\n",
       "      <td>banjo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3836 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                     timestamp      codes\n",
       "0       #‚É£ 2020-01-05 20:19:37.340117693  keycap: #\n",
       "1      #Ô∏è‚É£ 2020-01-05 20:19:37.340117693  keycap: #\n",
       "2       *‚É£ 2020-01-05 20:19:37.340117693  keycap: *\n",
       "3      *Ô∏è‚É£ 2020-01-05 20:19:37.340117693  keycap: *\n",
       "4       0‚É£ 2020-01-05 20:19:37.340117693  keycap: 0\n",
       "...    ...                           ...        ...\n",
       "3831     ü™ë 2020-01-05 20:19:37.340117693      chair\n",
       "3832     ü™í 2020-01-05 20:19:37.340117693      razor\n",
       "3833     ü™ì 2020-01-05 20:19:37.340117693        axe\n",
       "3834     ü™î 2020-01-05 20:19:37.340117693  diya lamp\n",
       "3835     ü™ï 2020-01-05 20:19:37.340117693      banjo\n",
       "\n",
       "[3836 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('C:/Users/mhmh2/.demoji/codes.json')\n",
    "df.reset_index(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   ÿ±ŸÇŸä  ÿßÿ®ÿØÿßÿßÿπ    ÿ∞ŸàŸàŸÇ   ŸÅÿÆÿßŸÖÿ©  ÿßŸÜÿßŸÇŸá.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"‚îä‚ú® ‚îä üíßÿ±ŸÇŸä ‚îäüçÉüåüüíßÿßÿ®ÿØÿßÿßÿßÿπ‚îäüåü‚îä‚ú®‚îäüåü‚îäüíßÿ∞ŸàŸàŸàŸÇ‚îäüçÉ‚îäüåü‚îäüíßŸÅÿÆÿßŸÖÿ©‚îä‚ú®üåü‚îäüíßÿßŸÜÿßŸÇŸá.\"\n",
    "# text =' ‚îä test'\n",
    "# text = ' tdg@bkja \"ŸÇŸàŸÑ ÿ®ÿ∞ŸÑÿ™ ÿ¨ŸáÿØŸä ŸàÿßŸÑÿ®ÿßŸÇŸä ÿπŸÑŸâ ÿßŸÑŸÑŸáÿü üòπüòπüòπüòπ https://t.co/YalhezFnsl \"#acsjjfac !\\n @ # % ^ & * ( üì¢‚úÖ‚ùéü•ò‚ÜòÔ∏èüåª‚ô•Ô∏è‚ô•Ô∏è‚ô•Ô∏èü•µüÜöüìÖüïóüìçüëãüò©üò¢üôåüèæüî•üòÆüíñüò≠üëÑ‚ù§ü§¢üí•üí£'\n",
    "text = delete_unnecessary_data(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Normalize whitespace (convert multiple sequential whitespace chars into one whitespace character). ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_whitespace(tweet):\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ÿ±ŸÇŸä ÿßÿ®ÿØÿßÿßÿπ ÿ∞ŸàŸàŸÇ ŸÅÿÆÿßŸÖÿ© ÿßŸÜÿßŸÇŸá.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = normalize_whitespace(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Convert hashtags into separate words, for example, thehashtag #MoroccanUsers is converted into two words Moroccan and Users. ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_h(hashtagestring):\n",
    "    fo = re.compile(r'#[A-Z]{2,}(?![a-z])|[A-Z][a-z]+')\n",
    "    fi = fo.findall(hashtagestring)\n",
    "    return ' '.join(fi)\n",
    "\n",
    "def split_hashtage(tweet):\n",
    "    tweet = re.sub(r'#[^\\s]+', lambda m: sp_h(m.group()), tweet) # #WakeUpMorocco => Wake Up Morocco\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata as ud\n",
    "import string\n",
    "\n",
    "def reamove_punctuation(tweet):\n",
    "#     regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "#     tweet = regex.sub('', tweet)\n",
    "#     tweet = re.sub(ud.category(c).startswith('P'),'',tweet)\n",
    "\n",
    "    tweet = re.sub('[^\\w\\s\\']',' ',tweet)\n",
    "#     tweet = ''.join(c for c in tweet if not ud.category(c).startswith('P'))\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) Create a function to detect the language used to write the text of tweet (Standard Arab, French or English). ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "from langdetect import detect\n",
    "from textblob import TextBlob\n",
    "\n",
    "def language_detction(tweet):\n",
    "    try:\n",
    "        lang = translator.detect(tweet).lang\n",
    "    except:\n",
    "        try:\n",
    "            lang = detect(tweet)\n",
    "        except:\n",
    "            try:\n",
    "                lang = str(TextBlob(tweet).detect_language())\n",
    "            except:\n",
    "                lang = 'unknown'\n",
    "    \n",
    "    if lang in ['ar', 'arfa', 'fa', 'faar'] :\n",
    "        tweet = correct_ar(tweet)\n",
    "    elif lang == 'fr':\n",
    "        tweet = correct_fr(tweet)\n",
    "    elif lang == 'en':\n",
    "        tweet = correct_en(tweet)\n",
    "    else:\n",
    "        tweet = translator.translate(tweet,src='auto',dest='en' ).text\n",
    "        tweet = correct_en(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhh je suis encore vivant \t\t hhh I'm still alive \t\t Detected(lang=fr, confidence=1.0)\n"
     ]
    }
   ],
   "source": [
    "# text = 'chkon nta'\n",
    "# text = \"ÿßÿ™ÿ®ÿßÿ±ŸÉ ÿßŸÑŸÑŸá ÿπŸÑŸâ ÿßŸÑÿ≠ÿßÿ¨ ÿ±ÿßŸÉ ÿ™ŸÖÿß.ŸáŸá\"\n",
    "text = 'hhh je suis encore vivant'\n",
    "# text ='ŸÉŸäŸÅ ŸäŸÖŸÉŸÜ ÿ™ÿπÿ≤Ÿäÿ≤ ŸÉŸÅÿßÿ°ÿßÿ™ Ÿáÿ∞ÿß ÿßŸÑÿ¨Ÿáÿßÿ≤ ÿ®ÿ™ÿ∑ŸàŸäÿ±Ÿá ÿ®ÿ¢ÿ≥ÿ™ÿÆÿ∞ÿßŸÖ ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿ•ÿµÿ∑ŸÜÿßÿπŸä ÿü ŸÖÿ´ŸÑÿß ŸäŸÖŸÉŸÜ ÿ•ÿ≥ÿ™ÿπŸÖÿßŸÑ ŸÇŸÑŸÖ Ÿäÿ∫Ÿäÿ± ŸÇÿ∑ÿ± ÿßŸÑŸÉÿ™ÿßÿ®ÿ© ÿ£Ÿà ÿ™ÿ∫ŸäŸäÿ± ŸÑŸàŸÜ ÿßŸÑÿ≠ÿ®ÿ± ÿ®ÿØŸàŸÜ ÿ™ÿ∫ŸäŸäÿ± ÿßŸÑŸÇŸÑŸÖ ŸÅŸÇÿ∑ ÿ®ÿ•ÿµÿØÿßÿ± ÿßŸÑÿ£ŸàÿßŸÖÿ±ÿå ŸÖÿßÿ∞ÿß ÿπŸÜŸÉŸÖ ÿü'\n",
    "dt = translator.detect(text)\n",
    "a = translator.translate(text,src='auto',dest='en' ).text\n",
    "print(text , \"\\t\\t\", a , \"\\t\\t\", dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f) Create a function for automatic correction of spelling mistakes. ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller\n",
    "spell = Speller(lang='en')\n",
    "\n",
    "def correct_en(text):\n",
    "    #convert to lower case\n",
    "    text = text.lower()\n",
    "    # Clean the text\n",
    "    text = re.sub(\"\\'s \", \" is\", text) # we have cases like \"Sam is\" or \"Sam's\" (i.e. his) these two cases aren't separable, I choose to compromise are kill \"'s\" directly\n",
    "    text = re.sub(r\" whats \", \" what is \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(\"n\\'t\", \" not \", text)\n",
    "    text = re.sub(\"i'm\", \"i am\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'re\", \" are \", text)\n",
    "    text = re.sub(\"\\'d\", \" would \", text)\n",
    "    text = re.sub(\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(\"e-mail\", \" email \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\(s\\)\", \" \", text, flags=re.IGNORECASE) #mester(s)\n",
    "    text = re.sub(r\" (the[\\s]+|The[\\s]+)?(us(a)?|u\\.s\\.(a\\.)?|united state(s)?) \", \" america \", text)\n",
    "    text = re.sub(r\" uk \", \" england \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" imrovement \", \" improvement \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" intially \", \" initially \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" dms \", \" direct messages \", text, flags=re.IGNORECASE)  \n",
    "    text = re.sub(r\" demonitization \", \" demonetization \", text, flags=re.IGNORECASE) \n",
    "    text = re.sub(r\" actived \", \" active \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" kms \", \" kilometers \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" cs \", \" computer science \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" calender \", \" calendar \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" ios \", \" operating system \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" programing \", \" programming \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" bestfriend \", \" best friend \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"bn8|god8\" ,'good night', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\" 2moro | 2mrrw | 2morrow | 2mrw | tomrw \", \" tomorrow \", text)\n",
    "    text = re.sub(r\" b4 \", \" before \", text)\n",
    "    text = re.sub(r\" otw \", \" on the way \", text)\n",
    "\n",
    "    text = spell(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def correct_fr(text):\n",
    "    text = TextBlob(text).correct()\n",
    "    return str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je manger le banana et la pot\n"
     ]
    }
   ],
   "source": [
    "print(correct_fr('je mangee le banane et la pom'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Transforming words written in Moroccan dialect ##\n",
    "\n",
    "(by the French alphabet or Arabic) and also written in a dialect of Berber Tamazight (by the French alphabet into the standard Arabic. \n",
    "\n",
    "For this reason, we create a dictionary of words that we gathered in a python file to perform this task, This file will be stored in each slave node of our cluster, and it will be imported in the NLP script executed in these nodes, the file looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Arabic</th>\n",
       "      <th>Transcribed Moroccan Arabic</th>\n",
       "      <th>Moroccan Darija in the Arabic Alphabet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes</td>\n",
       "      <td>ŸÜÿπŸÖ</td>\n",
       "      <td>Iyyeh</td>\n",
       "      <td>ÿ•ŸäŸäŸá</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>ŸÜÿπŸÖ</td>\n",
       "      <td>ah</td>\n",
       "      <td>ÿ¢Ÿá</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yes</td>\n",
       "      <td>ŸÜÿπŸÖ</td>\n",
       "      <td>wah</td>\n",
       "      <td>ŸàÿßŸá</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No</td>\n",
       "      <td>ŸÑÿß</td>\n",
       "      <td>Lla</td>\n",
       "      <td>ŸÑÿß</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Please</td>\n",
       "      <td>ŸÖŸÜ ŸÅÿ∂ŸÑŸÉ</td>\n",
       "      <td>3afak</td>\n",
       "      <td>ÿπÿßŸÅÿßŸÉ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Thanks</td>\n",
       "      <td>ÿ¥ŸÉÿ±ÿß</td>\n",
       "      <td>Shokran</td>\n",
       "      <td>ÿ¥ŸÉÿ±ÿß</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I love you</td>\n",
       "      <td>ÿßŸÜÿß ÿßÿ≠ÿ®ŸÉ</td>\n",
       "      <td>Kanbghik</td>\n",
       "      <td>ŸÉŸÜÿ®ÿ∫ŸäŸÉ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I miss you</td>\n",
       "      <td>ÿßÿ¥ÿ™ŸÇÿ™ ÿßŸÑŸäŸÉ</td>\n",
       "      <td>Twe77eshtek</td>\n",
       "      <td>ÿ™Ÿàÿ≠ÿ¥ÿ™ŸÉ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A lot</td>\n",
       "      <td>ŸÉÿ´Ÿäÿ±</td>\n",
       "      <td>Bezzaf</td>\n",
       "      <td>ÿ®ÿ≤ÿßŸÅ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A little</td>\n",
       "      <td>ŸÇŸÑŸäŸÑ</td>\n",
       "      <td>Shwiya</td>\n",
       "      <td>ÿ¥ŸàŸäÿ©</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      English      Arabic Transcribed Moroccan Arabic  \\\n",
       "0         Yes         ŸÜÿπŸÖ                       Iyyeh   \n",
       "1         Yes         ŸÜÿπŸÖ                          ah   \n",
       "2         Yes         ŸÜÿπŸÖ                         wah   \n",
       "3          No          ŸÑÿß                         Lla   \n",
       "4      Please     ŸÖŸÜ ŸÅÿ∂ŸÑŸÉ                       3afak   \n",
       "5      Thanks        ÿ¥ŸÉÿ±ÿß                     Shokran   \n",
       "6  I love you    ÿßŸÜÿß ÿßÿ≠ÿ®ŸÉ                    Kanbghik   \n",
       "7  I miss you  ÿßÿ¥ÿ™ŸÇÿ™ ÿßŸÑŸäŸÉ                 Twe77eshtek   \n",
       "8       A lot        ŸÉÿ´Ÿäÿ±                      Bezzaf   \n",
       "9    A little        ŸÇŸÑŸäŸÑ                      Shwiya   \n",
       "\n",
       "  Moroccan Darija in the Arabic Alphabet  \n",
       "0                                   ÿ•ŸäŸäŸá  \n",
       "1                                     ÿ¢Ÿá  \n",
       "2                                    ŸàÿßŸá  \n",
       "3                                     ŸÑÿß  \n",
       "4                                  ÿπÿßŸÅÿßŸÉ  \n",
       "5                                   ÿ¥ŸÉÿ±ÿß  \n",
       "6                                 ŸÉŸÜÿ®ÿ∫ŸäŸÉ  \n",
       "7                                 ÿ™Ÿàÿ≠ÿ¥ÿ™ŸÉ  \n",
       "8                                   ÿ®ÿ≤ÿßŸÅ  \n",
       "9                                   ÿ¥ŸàŸäÿ©  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ma_df = pd.read_csv( home + \"Moroccan_dialect.csv\", sep=';',encoding='utf-8')\n",
    "word_ma_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ahramon ÿßŸÑÿ∑ÿ® ÿßŸÑÿ®Ÿäÿ∑ÿ±Ÿä ŸÖŸÜ ŸÅÿ∂ŸÑŸÉ ŸÜÿπŸÖ ŸÜÿπŸÖ ÿ£ÿπÿ∑ŸÜŸä '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def moroccan_dialect(tweet):\n",
    "    tweet= ' '+tweet+' '\n",
    "    for ligne in word_ma_df.values:\n",
    "        tweet = re.sub(r' '+str(ligne[2])+' | '+str(ligne[3])+' ', ' '+str(ligne[1])+' ', tweet)\n",
    "    return tweet\n",
    "\n",
    "tweet = 'ahramon Ettebb elbaytari 3afak ÿ•ŸäŸäŸá wah 3tini'\n",
    "tweet = moroccan_dialect(tweet)\n",
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Use scrapy to scrape 'http://mylanguages.org/moroccan_vocabulary.php'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>ar</th>\n",
       "      <th>mafr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>numbers</td>\n",
       "      <td>ÿ£ÿ±ŸÇÿßŸÖ</td>\n",
       "      <td>nmari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>one</td>\n",
       "      <td>Ÿàÿßÿ≠ÿØ</td>\n",
       "      <td>wahade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>two</td>\n",
       "      <td>ÿßÿ´ŸÜÿßŸÜ</td>\n",
       "      <td>zouje</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>three</td>\n",
       "      <td>ÿ´ŸÑÿßÿ´ÿ©</td>\n",
       "      <td>tlata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>four</td>\n",
       "      <td>ÿ£ÿ±ÿ®ÿπÿ©</td>\n",
       "      <td>rabaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>five</td>\n",
       "      <td>ÿÆŸÖÿ≥ÿ©</td>\n",
       "      <td>khamssa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>six</td>\n",
       "      <td>ÿ≥ÿ™ÿ©</td>\n",
       "      <td>satta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a green tree</td>\n",
       "      <td>ÿ¥ÿ¨ÿ±ÿ© ÿÆÿ∂ÿ±ÿßÿ°</td>\n",
       "      <td>shajra khdra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a tall building</td>\n",
       "      <td>ŸÖÿ®ŸÜŸâ ÿ∑ŸàŸäŸÑ ÿßŸÑŸÇÿßŸÖÿ©</td>\n",
       "      <td>imara aliya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a very old man</td>\n",
       "      <td>ÿ±ÿ¨ŸÑ ŸÉÿ®Ÿäÿ± ÿ¨ÿØÿß</td>\n",
       "      <td>rajale  sharafe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                en                ar             mafr\n",
       "0          numbers             ÿ£ÿ±ŸÇÿßŸÖ            nmari\n",
       "1              one              Ÿàÿßÿ≠ÿØ           wahade\n",
       "2              two             ÿßÿ´ŸÜÿßŸÜ            zouje\n",
       "3            three             ÿ´ŸÑÿßÿ´ÿ©            tlata\n",
       "4             four             ÿ£ÿ±ÿ®ÿπÿ©            rabaa\n",
       "5             five              ÿÆŸÖÿ≥ÿ©          khamssa\n",
       "6              six               ÿ≥ÿ™ÿ©            satta\n",
       "7     a green tree        ÿ¥ÿ¨ÿ±ÿ© ÿÆÿ∂ÿ±ÿßÿ°     shajra khdra\n",
       "8  a tall building  ŸÖÿ®ŸÜŸâ ÿ∑ŸàŸäŸÑ ÿßŸÑŸÇÿßŸÖÿ©     imara aliya \n",
       "9   a very old man      ÿ±ÿ¨ŸÑ ŸÉÿ®Ÿäÿ± ÿ¨ÿØÿß  rajale  sharafe"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ma_df2 = pd.read_csv( home + \"mylanguages.csv\", sep=',',encoding='utf-8')\n",
    "word_ma_df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moroccan_dialect2(tweet):\n",
    "    tweet= ' '+tweet+' '\n",
    "    for ligne in word_ma_df.values:\n",
    "        tweet = re.sub(r' '+str(ligne[2])+' ', ' '+str(ligne[1])+' ', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ahramon ÿßŸÑÿ∑ÿ® ÿßŸÑÿ®Ÿäÿ∑ÿ±Ÿä ŸÖŸÜ ŸÅÿ∂ŸÑŸÉ ŸÜÿπŸÖ ŸÜÿπŸÖ ÿßÿπÿ∑ŸÜŸä '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from https://alraqmiyyat.github.io/2013/01-02.html\n",
    "def normalizeArabic(text):\n",
    "    text = re.sub(\"[ÿ•ÿ£Ÿ±ÿ¢ÿß]\", \"ÿß\", text)\n",
    "    text = re.sub(\"Ÿâ\", \"Ÿä\", text)\n",
    "    text = re.sub(\"ÿ§\", \"ÿ°\", text)\n",
    "    text = re.sub(\"ÿ¶\", \"ÿ°\", text)\n",
    "    return(text)\n",
    "\n",
    "normalizeArabic(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÿ•ŸÜ ÿßŸÑŸÇÿ±ÿßÿ° ŸäŸÇÿ±ÿ§ŸàŸÜ ÿßŸÑŸÇÿ±ÿ¢ŸÜ ŸÇÿ±ÿßÿ°ÿ© ÿ¨ŸÖŸäŸÑÿ©\n"
     ]
    }
   ],
   "source": [
    "# from https://alraqmiyyat.github.io/2013/01-02.html\n",
    "def deNoise(text):\n",
    "    noise = re.compile(\"\"\" Ÿë    | # Tashdid\n",
    "                             Ÿé    | # Fatha\n",
    "                             Ÿã    | # Tanwin Fath\n",
    "                             Ÿè    | # Damma\n",
    "                             Ÿå    | # Tanwin Damm\n",
    "                             Ÿê    | # Kasra\n",
    "                             Ÿç    | # Tanwin Kasr\n",
    "                             Ÿí    | # Sukun\n",
    "                             ŸÄ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "    text = re.sub(noise, '', text)\n",
    "    return text\n",
    "\n",
    "testLine = \"ÿ•ŸêŸÜŸéŸë ÿßŸÑŸíŸÇŸèÿ±ŸéŸëÿßŸíÿ°Ÿé ŸäŸéŸÇŸíÿ±Ÿéÿ§ŸèŸàŸíŸÜŸé ÿßŸÑŸíŸÇŸèÿ±Ÿíÿ¢ŸÜŸé ŸÇŸêÿ±ŸéÿßŸíÿ°Ÿéÿ©Ÿã ÿ¨ŸéŸÖŸêŸäŸíŸÑŸéŸÄŸÄŸÄŸÄŸÄŸÄÿ©Ÿã\"\n",
    "print(deNoise(testLine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyarabic'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-d4820944f6bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyarabic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maraby\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0maraby\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyarabic'"
     ]
    }
   ],
   "source": [
    "import pyarabic.araby as araby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_ar(text):\n",
    "    text = moroccan_dialect(text)\n",
    "    text = moroccan_dialect2(text)\n",
    "    text = normalizeArabic(text)\n",
    "    text = deNoise(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('arabic')\n",
    "with open(\"arabic_stop_words.txt\",\"r\", newline=\"\",encoding=\"utf-8\") as f:        \n",
    "    for l in f:\n",
    "        l = re.sub(r\"\\n+\",'',l)\n",
    "        stop_words.append(l)\n",
    "\n",
    "stop_words.extend(stopwords.words('french'))\n",
    "stop_words.extend(stopwords.words('english'))\n",
    "stop_words.extend(gensim.parsing.preprocessing.STOPWORDS)\n",
    "stop_words.extend(['tout','le','la'])\n",
    "stop_words = set(stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(lmtzr.lemmatize(text, pos='v'))\n",
    "\n",
    "\n",
    "def tokenize_lemmatize_stemming(text):\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    #replace multiple spaces with one space\n",
    "    text = re.sub(r'[\\s]+',' ',text)\n",
    "    #transfer text to lowercase\n",
    "    text = text.lower() \n",
    "    # tokenaze text\n",
    "    tokens = re.split(\" \", text)\n",
    "\n",
    "    # Remove stop words \n",
    "    result = []\n",
    "    for token in tokens :\n",
    "        if token not in stop_words and len(token) > 1:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweet):\n",
    "    tweet = delete_unnecessary_data(tweet)\n",
    "    tweet = normalize_whitespace(tweet)\n",
    "    tweet = split_hashtage(tweet)\n",
    "    tweet = reamove_punctuation(tweet)\n",
    "    tweet = language_detction(tweet)\n",
    "#     return tweet\n",
    "    tokens = tokenize_lemmatize_stemming(tweet)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"‚îä‚ú® ‚îä üíßÿ±ŸÇŸä ‚îäüçÉüåüüíßÿßÿ®ÿØÿßÿßÿßÿπ‚îäüåü‚îä‚ú®‚îäüåü‚îäüíßÿ∞ŸàŸàŸàŸÇ‚îäüçÉ‚îäüåü‚îäüíßŸÅÿÆÿßŸÖÿ©‚îä‚ú®üåü‚îäüíßÿßŸÜÿßŸÇŸá.\"\n",
    "# text = '   ÿ±ŸÇŸä  ÿßÿ®ÿØÿßÿßÿπ    ÿ∞ŸàŸàŸÇ   ŸÅÿÆÿßŸÖÿ©  ÿßŸÜÿßŸÇŸá.'\n",
    "text =' RT @PsiliLemonia: Œ†œéœÇ ŒΩŒ± Œ≤Œ¨ŒªŒµŒπœÇ œÜœâœÑŒπŒ¨ œÉœÑŒø Œ∫œÅŒµŒ≤Œ¨œÑŒπ œÉŒ±œÇ ŒºŒµ ŒµœÜœÑŒ¨ Œ±œÄŒªŒ≠œÇ Œ∫ŒπŒΩŒÆ'\n",
    "# text = \"Se realmente quisessem estar do meu lado iam estar, e quem t√° j√° basta viu!\"\n",
    "# text = preprocess(\"ŸÇŸàŸÑ ÿ®ÿ∞ŸÑÿ™ ÿ¨ŸáÿØŸä ŸàÿßŸÑÿ®ÿßŸÇŸä ÿπŸÑŸâ ÿßŸÑŸÑŸáÿü üòπüòπüòπüòπ https://t.co/YalhezFnsl \")\n",
    "text = preprocess(text)\n",
    "# print(delete_unnecessary_data(\"ŸÇŸàŸÑ ÿ®ÿ∞ŸÑÿ™ ÿ¨ŸáÿØŸä ŸàÿßŸÑÿ®ÿßŸÇŸä ÿπŸÑŸâ ÿßŸÑŸÑŸáÿü üòπüòπüòπüòπ https://t.co/YalhezFnsl \"))\n",
    "print(text)\n",
    "\n",
    "# dt = translator.detect(text).lang\n",
    "\n",
    "# tweet = translator.translate(text,src='auto',dest='en' ).text\n",
    "# print(text , \"\\n\",dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv(home+'tweets2.csv',encoding='utf-8')\n",
    "tweets_df.columns = ['user','date','text']\n",
    "tweets_df.astype({'text': str})\n",
    "\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = tweets_df['text'][1]\n",
    "# tweet = '@jawaddelycee Bah ouais logique bg aaaaaaa'\n",
    "preprocess(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = []\n",
    "# for tweet in tqdm(tweets_df['text']):\n",
    "#     tokens.append(preprocess(tweet))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.machinelearningplus.com/nlp/gensim-tutorial/\n",
    "\n",
    "## How to create Topic Models with LDA?\n",
    "\n",
    "<p>\n",
    "The objective of topic models is to extract the underlying topics from a given collection of text documents. Each document in the text is considered as a combination of topics and each topic is considered as a combination of related words.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Topic modeling can be done by algorithms like Latent Dirichlet Allocation (LDA) and Latent Semantic Indexing (LSI).\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "In both cases you need to provide the number of topics as input. The topic model, in turn, will provide the topic keywords for each topic and the percentage contribution of topics in each document.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "The quality of topics is highly dependent on the quality of text processing and the number of topics you provide to the algorithm. The earlier post on how to build best topic models explains the procedure in more detail. However, I recommend understanding the basic steps involved and the interpretation in the example below.\n",
    "</p>\n",
    "\n",
    "\n",
    "### Step 0: Load the necessary packages and import the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
