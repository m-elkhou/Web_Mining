# -*- coding: utf-8 -*-
"""final_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rVsdn7Ib_71yZoZoKUyE0PGDqNqxlA4d

# Web Mining - Project : #

***

## Objectif : 

<p>
Mining social media network (FaceBook, Twitter, etc.) and news websites </br>
for discovering what topics Moroccan people are discussing during the two last years.  

- Use LDA (Latent Dirichlet Allocation) or other topic identification techniques.

- Provide a deep analysis.
</p>

***
</br>

### Importation des libraires :
"""

# from google.colab import drive
# drive.mount("/content/drive/")

# !pip install -U -r '/content/drive/My Drive/Colab Notebooks/Current_Trends_in_Moroccan_Social_Networks/requirements.txt'

import numpy as np
import pandas as pd 
import re
# from tqdm.notebook import tqdm
from tqdm import tqdm
from googletrans import Translator
from langdetect import detect
from textblob import TextBlob

import gensim
from gensim.parsing.preprocessing import STOPWORDS
from nltk.corpus import stopwords

home = 'D:/WISD/S3/Web_Mining/Current_Trends_in_Moroccan_Social_Networks/'
# home = '/content/drive/My Drive/Colab Notebooks/Current_Trends_in_Moroccan_Social_Networks/'


"""# Processing tweets with NLP #

***

#### *The steps we followed to process the stored Tweets are:*

- *a)* Delete unnecessary data: usernames,emails,hyperlinks, retweets, punctuation, 
possessives from a noun,duplicate characters, and special characters like smileys.

- *b)* Normalize whitespace (convert multiple sequential whitespace chars into one whitespace character).

- *c)* Convert hashtags into separate words, for example, thehashtag #MoroccanUsers 
is converted into two wordsMoroccan and Users.

- *d)* Transform words writtenin Moroccan dialect, or in a dialect of Berber Tamazight into Standard Arabic. 
These words couldbe written using the Arabic or French alphabet. 
To performthistask, we create a python file that contains a dictionary of words that we gathered, 
then we store it in each slave node of our clusterand imported inside the NLP script

- *e)* Create a function to detect the language used to write
the text of tweet (Standard Arab, French or English).

- *f)* Create a function for automatic correction of spelling mistakes.

- *g)* Create a list of contractions to normalize and
expandwordslike What's=>What is

- *h)* Delete the suffix of a word until we find the root. For
example; Stemming => stem

- *i)* Remove stopwords for standard Arabic ( ,(...,ÿ£ŸÜ, ÿ•ŸÜ, ÿ®ÿπÿØ
French (alors, √†, ainsi, ...), and English (about, above, almost,...).

## a) Delete unnecessary data: usernames, emails, hyperlinks, punctuation, duplicate characters, and special characters like smileys (emoji). ##
"""
print('[LODING]')
demoji_df = pd.read_json(home + 'demoji.json', encoding='utf-8') # from https://pypi.org/project/demoji/
demoji_df.reset_index(inplace=True)
demoji_df.astype({"index": str})

def delete_unnecessary_data(tweet):
    #delete www.* or https?://* 
    tweet = re.sub('((www\.[^\s]+)|(https?://[^\s]+))','',tweet)
    #delete @username
    tweet = re.sub(' @[^\s]+',' ',tweet)
    #delete username*
    tweet = re.sub('username[^\s]+','',tweet)
    #delete emails
    tweet = re.sub('[^\s@]+@[^\s@]+',' ',tweet)

    #delete javascript tags
    tweet =re.sub('< *script*>.*?< *script*>',' ',tweet)
    #delete all html tags
    tweet = re.sub('<.*?>',' ',tweet)
    #delete numbers 
    tweet = re.sub("[0-9><,]+"," ",tweet)
    #delete reteur a la ligne
    tweet = re.sub(r"\n+|‚îä"," ",tweet)

    # delete duplicate characters
    tweet = re.sub(r"(.)\1{2,}", r"\1\1", tweet)
    '''
    # Remove special characters like smileys (imojie)
    üòÇü§∑‚Äç‚ôÄÔ∏è‚ù§Ô∏èüî¥üì¢‚úÖ‚ùéü•ò‚ÜòÔ∏èüåª‚ô•Ô∏è‚ô•Ô∏è‚ô•Ô∏èü•µüÜöüìÖüïóüìçüëãüò©üò¢üôåüèæüî•üòÆüíñüò≠üëÑ‚ù§ü§¢üí•üí£
    üéÑ‚ù§‚ù§üê™üê±üí∞üè∑‚≠êüôÑüòçüôåüëáüíöüò≠üòπüå∏üíõüôèüëèüòîüéÅü•∞‚ùÑüéÑüí§
    '''     
    # tweet = re.sub(r'üòÇ|ü§∑‚Äç‚ôÄÔ∏è|‚ù§Ô∏è|üî¥|üì¢|‚úÖ|‚ùé|ü•ò|‚ÜòÔ∏è|üåª|‚ô•Ô∏èÔ∏è|Ô∏èü•µ|üÜö|üìÖ|üïó|üìç|üëã|üò©|üò¢|üôåüèæ|üî•|üòÆ|üíñ|üò≠|üëÑ|‚ù§|ü§¢|üí•|üí£','',tweet)
    # Stage el wa7che
    # tweet = tweet.encode('ascii', 'ignore').decode('ascii')
    
    for code in demoji_df["index"]:
        try:
            tweet = re.sub(code, '', tweet)
        except:
            pass
        
    return tweet


"""## b) Normalize whitespace (convert multiple sequential whitespace chars into one whitespace character). ##"""

def normalize_whitespace(tweet):
    tweet = re.sub('[\s]+', ' ', tweet)
    return tweet

"""## c) Convert hashtags into separate words, for example, thehashtag #MoroccanUsers is converted into two words Moroccan and Users. ##"""

def sp_h(hashtagestring):
    fo = re.compile(r'#[A-Z]{2,}(?![a-z])|[A-Z][a-z]+')
    fi = fo.findall(hashtagestring)
    return ' '.join(fi)

def split_hashtage(tweet):
    tweet = re.sub(r'#[^\s]+', lambda m: sp_h(m.group()), tweet) # #WakeUpMorocco => Wake Up Morocco
    return tweet

def reamove_punctuation(tweet):
    tweet = re.sub('[^\w\s\']',' ',tweet)
    return tweet

"""## e) Create a function to detect the language used to write the text of tweet (Standard Arab, French or English). ##"""

from googletrans import Translator
translator = Translator()
from langdetect import detect
from textblob import TextBlob

def language_detction(tweet):
    try:
        lang = translator.detect(tweet).lang
    except:
        try:
            lang = detect(tweet)
        except:
            try:
                lang = str(TextBlob(tweet).detect_language())
            except:
                lang = 'unknown'
    
    if lang in ['ar', 'arfa', 'fa', 'faar'] :
        tweet = correct_ar(tweet)
    elif lang == 'fr':
        tweet = correct_fr(tweet)
    elif lang == 'en':
        tweet = correct_en(tweet)
    else:
        tweet = translator.translate(tweet,src='auto',dest='en' ).text
        tweet = correct_en(tweet)
    return tweet

"""## f) Create a function for automatic correction of spelling mistakes. ##"""

from autocorrect import Speller
spell = Speller(lang='en')

def correct_en(text):
    #convert to lower case
    text = text.lower()
    # Clean the text
    text = re.sub("\'s ", " is", text) # we have cases like "Sam is" or "Sam's" (i.e. his) these two cases aren't separable, I choose to compromise are kill "'s" directly
    text = re.sub(r" whats ", " what is ", text, flags=re.IGNORECASE)
    text = re.sub("\'ve", " have ", text)
    text = re.sub("n\'t", " not ", text)
    text = re.sub("i'm", "i am", text, flags=re.IGNORECASE)
    text = re.sub("\'re", " are ", text)
    text = re.sub("\'d", " would ", text)
    text = re.sub("\'ll", " will ", text)
    text = re.sub("e-mail", " email ", text, flags=re.IGNORECASE)
    text = re.sub("\(s\)", " ", text, flags=re.IGNORECASE) #mester(s)
    text = re.sub(r" (the[\s]+|The[\s]+)?(us(a)?|u\.s\.(a\.)?|united state(s)?) ", " america ", text)
    text = re.sub(r" uk ", " england ", text, flags=re.IGNORECASE)
    text = re.sub(r" imrovement ", " improvement ", text, flags=re.IGNORECASE)
    text = re.sub(r" intially ", " initially ", text, flags=re.IGNORECASE)
    text = re.sub(r" dms ", " direct messages ", text, flags=re.IGNORECASE)  
    text = re.sub(r" demonitization ", " demonetization ", text, flags=re.IGNORECASE) 
    text = re.sub(r" actived ", " active ", text, flags=re.IGNORECASE)
    text = re.sub(r" kms ", " kilometers ", text, flags=re.IGNORECASE)
    text = re.sub(r" cs ", " computer science ", text, flags=re.IGNORECASE)
    text = re.sub(r" calender ", " calendar ", text, flags=re.IGNORECASE)
    text = re.sub(r" ios ", " operating system ", text, flags=re.IGNORECASE)
    text = re.sub(r" programing ", " programming ", text, flags=re.IGNORECASE)
    text = re.sub(r" bestfriend ", " best friend ", text, flags=re.IGNORECASE)
    text = re.sub(r"bn8|god8" ,'good night', text, flags=re.IGNORECASE)
    text = re.sub(r" 2moro | 2mrrw | 2morrow | 2mrw | tomrw ", " tomorrow ", text)
    text = re.sub(r" b4 ", " before ", text)
    text = re.sub(r" otw ", " on the way ", text)

    text = spell(text)

    return text

from textblob import TextBlob

def correct_fr(text):
    text = TextBlob(text).correct()
    return str(text)


"""## d) Transforming words written in Moroccan dialect ##

(by the French alphabet or Arabic) and also written in a dialect of Berber Tamazight (by the French alphabet into the standard Arabic. 

For this reason, we create a dictionary of words that we gathered in a python file to perform this task, This file will be stored in each slave node of our cluster, and it will be imported in the NLP script executed in these nodes, the file looks like
"""

word_ma_df = pd.read_csv( home + "Moroccan_dialect.csv", sep=';',encoding='utf-8')

def moroccan_dialect(tweet):
    tweet= ' '+tweet+' '
    for ligne in word_ma_df.values:
        tweet = re.sub(r' '+str(ligne[2])+' | '+str(ligne[3])+' ', ' '+str(ligne[1])+' ', tweet)
    return tweet

"""Use scrapy to scrape the website : 'http://mylanguages.org/moroccan_vocabulary.php'"""

word_ma_df2 = pd.read_csv( home + "mylanguages.csv", sep=',',encoding='utf-8')
# word_ma_df2

def moroccan_dialect2(tweet):
    tweet= ' '+tweet+' '
    for ligne in word_ma_df.values:
        tweet = re.sub(r' '+str(ligne[2])+' ', ' '+str(ligne[1])+' ', tweet)
    return tweet

# from https://alraqmiyyat.github.io/2013/01-02.html
def normalizeArabic(text):
    text = re.sub("[ÿ•ÿ£Ÿ±ÿ¢ÿß]", "ÿß", text)
    text = re.sub("Ÿâ", "Ÿä", text)
    text = re.sub("ÿ§", "ÿ°", text)
    text = re.sub("ÿ¶", "ÿ°", text)
    return(text)

# from https://alraqmiyyat.github.io/2013/01-02.html
def deNoise(text):
    noise = re.compile(""" Ÿë    | # Tashdid
                             Ÿé    | # Fatha
                             Ÿã    | # Tanwin Fath
                             Ÿè    | # Damma
                             Ÿå    | # Tanwin Damm
                             Ÿê    | # Kasra
                             Ÿç    | # Tanwin Kasr
                             Ÿí    | # Sukun
                             ŸÄ     # Tatwil/Kashida
                         """, re.VERBOSE)
    text = re.sub(noise, '', text)
    return text

def correct_ar(text):
    text = moroccan_dialect(text)
    text = moroccan_dialect2(text)
    text = normalizeArabic(text)
    text = deNoise(text)
    
    return text

"""## import the stopwords ##"""

import gensim
from gensim.parsing.preprocessing import STOPWORDS
from nltk.corpus import stopwords
import nltk
# nltk.download('stopwords')

stop_words = stopwords.words('arabic')
with open(home+"arabic_stop_words.txt","r", newline="",encoding="utf-8") as f:        
    for l in f:
        l = re.sub(r"\n+",'',l)
        stop_words.append(l)

stop_words.extend(stopwords.words('french'))
stop_words.extend(stopwords.words('english'))
stop_words.extend(gensim.parsing.preprocessing.STOPWORDS)
stop_words.extend(['tout','le','la'])
stop_words = set(stop_words)

"""## lemmatize stemming tokenization ##"""

import nltk
# nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer

stemmer = PorterStemmer()
lmtzr = WordNetLemmatizer()

def lemmatize_stemming(text):
    return stemmer.stem(lmtzr.lemmatize(text, pos='v'))


def tokenize_lemmatize_stemming(text):
    text = re.sub(r'[^\w\s]','',text)
    #replace multiple spaces with one space
    text = re.sub(r'[\s]+',' ',text)
    #transfer text to lowercase
    text = text.lower() 
    # tokenaze text
    tokens = re.split(" ", text)

    # Remove stop words 
    result = []
    for token in tokens :
        if token not in stop_words and len(token) > 1:
            result.append(lemmatize_stemming(token))

    return result

"""## Globale Fonction"""

def preprocess(tweet):
  try:
    tweet = delete_unnecessary_data(tweet)
    tweet = normalize_whitespace(tweet)
    tweet = split_hashtage(tweet)
    tweet = reamove_punctuation(tweet)
    tweet = language_detction(tweet)
    tokens = tokenize_lemmatize_stemming(tweet)

    with open(home+"clean_data.txt", "a", encoding="utf-8") as f:
      f.writelines(','.join(tokens)+'\n')
  except:
    tokens = []

  return tokens

"""# Load The Data #

***

## twitter Data set scaraping with twepy
"""

columns = ['user','date','text']
tweets_df  = pd.read_csv(home+'twitter/tweets.csv',encoding='utf-8')
tweets_df.columns = columns
tweets_df2 = pd.read_csv(home+'twitter/tweets2.csv',encoding='utf-8')
tweets_df2.columns = columns
tweets_df3 = pd.read_csv(home+'twitter/tweets3.csv',encoding='utf-8')
tweets_df3.columns = columns
tweets_df = pd.concat([tweets_df3, tweets_df2, tweets_df])

tweets_df.astype({'text': str, 'date': str, 'text': str, 'user': str})
tweets_df = tweets_df.drop_duplicates()
tweets_df.reset_index(drop = True, inplace=True)
print('[START tw]')
import sys
for text in tqdm(tweets_df['text'][2100+41419+52606:], desc='TW', file=sys.stdout): 
    preprocess(text)

##################################################################################################################################################
"""## Hespress dataset scraping withe Scrapy"""

hespress_df  = pd.read_csv(home+'hespress/HespressComments.csv',encoding='utf-8') # 2019
# hespress_df.astype({'text': str, 'date': str, 'text': str, 'user': str})
hespress_df = hespress_df.drop_duplicates()
hespress_df.reset_index(drop = True, inplace = True)
print('[START hp]')
import sys
for text in tqdm(hespress_df['comment'], desc='HP', file=sys.stdout):
    preprocess(text)
# hespress_df

# text = hespress_df['comment'][0]
# preprocess(text)

"""## Facebook Data scraped with"""

facebook_df  = pd.read_csv(home+'facebook/facebookPosts.csv',encoding='utf-8')
print('[START fb]')
import sys
for text in tqdm(facebook_df['text'], desc='FB', file=sys.stdout):
    preprocess(text)
# facebook_df

# df = pd.concat([tweets_df['text'],  hespress_df['comment'],  hespress_df['titel'], facebook_df['text']])
# df

# Create and register a new `tqdm` instance with `pandas`
# tqdm.pandas()

# df.apply(lambda x: preprocess(x))
# df1 = df.progress_apply(lambda x: preprocess(x))

# df.save_csv(home+"clean_data.csv")
# df1.save_csv(home+"clean_data1.csv")
print('[DONE]')
